{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14035,
     "status": "ok",
     "timestamp": 1709409209774,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "_H0fT4oWeXMu",
    "outputId": "0b747e41-3ced-4e7c-afdf-7dd640937fc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: datasets==2.16.1 in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
      "Collecting accelerate==0.26.1\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
      "Requirement already satisfied: trl==0.7.11 in /usr/local/lib/python3.10/dist-packages (0.7.11)\n",
      "Requirement already satisfied: peft==0.8.2 in /usr/local/lib/python3.10/dist-packages (0.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.9.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (2.1.0+cu121)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.42.0) (1.11.4)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl==0.7.11) (0.7.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (2.1.0)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.7.11) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.7.11) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.7.11) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (0.1.2)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.27.2\n",
      "    Uninstalling accelerate-0.27.2:\n",
      "      Successfully uninstalled accelerate-0.27.2\n",
      "Successfully installed accelerate-0.26.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers==4.38.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\"\n",
    "!pip install huggingface_hub langchain accelerate --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38825,
     "status": "ok",
     "timestamp": 1709409251652,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "wiAD_SEOuOB_",
    "outputId": "7fdc81eb-a6f9-47f7-d160-7b40ba6dbf49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'FastChat'...\n",
      "remote: Enumerating objects: 5847, done.\u001b[K\n",
      "remote: Total 5847 (delta 0), reused 0 (delta 0), pack-reused 5847\u001b[K\n",
      "Receiving objects: 100% (5847/5847), 31.83 MiB | 16.05 MiB/s, done.\n",
      "Resolving deltas: 100% (4346/4346), done.\n",
      "Obtaining file:///content/FastChat\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.9.3)\n",
      "Collecting fastapi (from fschat==0.2.36)\n",
      "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx (from fschat==0.2.36)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown2[all] (from fschat==0.2.36)\n",
      "  Downloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nh3 (from fschat==0.2.36)\n",
      "  Downloading nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (1.25.2)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.0.43)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.6.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (5.9.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.31.0)\n",
      "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (13.7.0)\n",
      "Collecting shortuuid (from fschat==0.2.36)\n",
      "  Downloading shortuuid-1.0.12-py3-none-any.whl (10 kB)\n",
      "Collecting tiktoken (from fschat==0.2.36)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn (from fschat==0.2.36)\n",
      "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai<1 (from fschat==0.2.36)\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anthropic>=0.3 (from fschat==0.2.36)\n",
      "  Downloading anthropic-0.16.0-py3-none-any.whl (846 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.4/846.4 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ray (from fschat==0.2.36)\n",
      "  Downloading ray-2.9.3-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.21 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.27.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.8.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.1.99)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.1.0+cu121)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (4.38.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.21.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic>=0.3->fschat==0.2.36) (1.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic>=0.3->fschat==0.2.36) (4.10.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx->fschat==0.2.36)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (3.6)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->fschat==0.2.36)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<1->fschat==0.2.36) (4.66.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat==0.2.36) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat==0.2.36) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->fschat==0.2.36) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.36) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.36) (2.0.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.36) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.36) (2.16.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fschat==0.2.36) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->fschat==0.2.36) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (4.0.3)\n",
      "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->fschat==0.2.36)\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wavedrom (from markdown2[all]->fschat==0.2.36)\n",
      "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray->fschat==0.2.36) (8.1.7)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->fschat==0.2.36) (4.19.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->fschat==0.2.36) (1.0.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic>=0.3->fschat==0.2.36) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.36) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fschat==0.2.36) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->fschat==0.2.36) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.18.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fschat==0.2.36) (1.3.0)\n",
      "Collecting svgwrite (from wavedrom->markdown2[all]->fschat==0.2.36)\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.16.0)\n",
      "Building wheels for collected packages: fschat, wavedrom\n",
      "  Building editable for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fschat: filename=fschat-0.2.36-0.editable-py3-none-any.whl size=14311 sha256=924c208f5157ff980741a653c5bb6fa4ed4b832b0e4a5744612ae52109237057\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1v114c9p/wheels/0e/7d/3f/6256e4d259fdebc8665545ea33ca3112027cecb15f3a295311\n",
      "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30052 sha256=125fecefd1bc8857ecdd8d5f7074e3962d7cb325594d6d14b2bd06dc1cb9abc1\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
      "Successfully built fschat wavedrom\n",
      "Installing collected packages: nh3, svgwrite, shortuuid, markdown2, h11, wavedrom, uvicorn, tiktoken, starlette, httpcore, openai, httpx, fastapi, ray, fschat, anthropic\n",
      "Successfully installed anthropic-0.16.0 fastapi-0.110.0 fschat-0.2.36 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 markdown2-2.4.13 nh3-0.2.15 openai-0.28.1 ray-2.9.3 shortuuid-1.0.12 starlette-0.36.3 svgwrite-1.4.3 tiktoken-0.6.0 uvicorn-0.27.1 wavedrom-2.0.3.post3\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Clone main branch of FastChat\n",
    "!git clone https://github.com/philschmid/FastChat.git\n",
    "# Install FastChat with model worker and llm_judge dependencies\n",
    "!pip install -e \"./FastChat[model_worker,llm_judge]\"\n",
    "!pip install matplotlib tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "4f448c0b115047bb95823e2793e0cb80",
      "bf2bd3529398440a8ac4a10e44e3eb3d",
      "6dc5db10fb494414b301cc848a55a7a1",
      "04ab59bc6a4c42fc9a00f982b86f4ce5",
      "46481ee5621a42708d58eaab50795b5b",
      "10da2929d4854ef3bb2e472b264e6d43",
      "cca4b067586e483593154a0e95d035ee",
      "0c743f33a98e4df4b82443798c68e079",
      "22aed5cd75734b9ca913770a558967ef",
      "4f0e2689687c42b5b6e6b2f912c791b0",
      "e31d0391df1e46639a11a9ad2be849b2"
     ]
    },
    "executionInfo": {
     "elapsed": 17771,
     "status": "ok",
     "timestamp": 1709409313524,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "PsFD5agJeehK",
    "outputId": "6f5aacb3-39a9-4eb3-fb5e-954e0050029a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f448c0b115047bb95823e2793e0cb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient, login\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# access token with permission to access the model and PRO subscription\n",
    "hf_token = HF_token # <https://huggingface.co/settings/tokens>\n",
    "login(token=hf_token)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline\n",
    "\n",
    "peft_model_id = \"jinhybr/gemma-7b-Dolly15k-chatml\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# get token id for end of conversation\n",
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1709405774201,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "VCIcKmbUe3lc"
   },
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19519,
     "status": "ok",
     "timestamp": 1709405823348,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "sEiNI-THhhnD",
    "outputId": "afaa47bb-858c-425d-f2df-0126883e0cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Biden is the current president of United States. He is the 46th president of United States. He is a member of democratic party and the 78th and the current president of United States. He is also the vice president during the Obama administration for 8 years. His term as the president is from 2021.  He is a native of Delaware.  His first name is Joseph and his surname is Biden. His official twitter handle is @JoeBiden.  His first name is spelled as Joe and his surname is spelled as Biden.  He is a native of Delaware.  He is married to Jill Biden.  He has two children, Ashley Biden and Hunter Biden.  Hunter Biden is the son and he is a lawyer by profession.  He is also the former second son of the United States.  Hunter Biden is in the news lately because of his association with China.  Hunter Biden has been in the news for his addiction to drugs and alcohol.  Hunter Biden is also a father to 5 children.  Hunter Biden has been in the news for his association with China and his addiction to drugs and alcohol.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is the current president of United States?\"\n",
    "pred = test_inference(prompt)\n",
    "print(pred)\n",
    "# The current president of the United States is Joe Biden. ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIL0fSJPmVy6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3304,
     "status": "ok",
     "timestamp": 1709408139438,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "u3yB7eOHfOfC"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Dolly Dataset.\n",
    "dataset = load_dataset(\"Open-Orca/SlimOrca\", split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13619,
     "status": "ok",
     "timestamp": 1709408178003,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "_eViEF0Fp0uo",
    "outputId": "a7f77324-c3d6-4f03-8c4d-7af86c8c7e8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'system',\n",
       "  'value': 'You are an AI assistant that follows instruction extremely well. Help as much as you can.',\n",
       "  'weight': None},\n",
       " {'from': 'human',\n",
       "  'value': 'Answer the following question: - number is 54    - debutteam is pittsburgh steelers    - draftpick is 166      - birth date is 24 may 1982    - weight is 243    - nfl is wal475737    - debutyear is 2005    - finalteam is new york sentinels    - statlabel is tackles sacks interceptions    - heightin is 3    - statvalue is 9 0.0 1    - heightft is 6    - college is temple    - birth place is pottstown , pennsylvania    - draftyear is 2005    - position is linebacker    - draftround is 5    - finalyear is 2009    Given the details above, guess who could this information be about.\\nAnswer:',\n",
       "  'weight': 0.0},\n",
       " {'from': 'gpt',\n",
       "  'value': 'The information provided seems to refer to Rian Wallace, a former NFL player.',\n",
       "  'weight': 1.0}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['conversations'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1709408456757,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "nbIUcJ6oqlgF"
   },
   "outputs": [],
   "source": [
    "prompt = 'Answer the following question: - number is 54    - debutteam is pittsburgh steelers    - draftpick is 166      - birth date is 24 may 1982    - weight is 243    - nfl is wal475737    - debutyear is 2005    - finalteam is new york sentinels    - statlabel is tackles sacks interceptions    - heightin is 3    - statvalue is 9 0.0 1    - heightft is 6    - college is temple    - birth place is pottstown , pennsylvania    - draftyear is 2005    - position is linebacker    - draftround is 5    - finalyear is 2009    Given the details above, guess who could this information be about.\\nAnswer:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1709408466510,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "UKh-IbkorwYj",
    "outputId": "fdbd2657-7899-4d35-d3f0-6c9737ca1e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This could be about Troy Polamalu, the famous NFL player.'\n"
     ]
    }
   ],
   "source": [
    "pred = test_inference(prompt)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1709408622625,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "SarVlHiWry2U"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle().select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1709408720223,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "w0vytWp4sZSe",
    "outputId": "d0e524e2-2493-4fc8-e309-0b32b022e748"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"It seems as if I slept a few minutes ago , had an eventful dream and then was impolitely woken up . That 's how these 11 weeks at Redmond have whooshed past . And at this point , it 's still unclear to me how so many experiences and so many memories were registered in a single summer .\\n\\nOptions are: A). None of the above choices .; B). The summer was so exciting it seems like I lived a lot during the summer months .; C). I was so tightly packed that I forgot .; D). My clothes were packed in tight cases and it was disconcerting .;\\nAnswer the following question: What is the reason that it was unclear about there being so many memories in a summer ?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['conversations'][1][1]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1709408756692,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "B9hmZ1I-sb7s",
    "outputId": "d4f31088-6996-48a4-a6aa-2fd4d32c4f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'It is not clear because there are so many memories in a summer.'\n"
     ]
    }
   ],
   "source": [
    "prompt = dataset['conversations'][1][1]['value']\n",
    "pred = test_inference(prompt)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1709408785361,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "0rQObaVZs5x2",
    "outputId": "2f6b40bb-0b13-40b3-e064-b933a87a1d98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 'gpt',\n",
       " 'value': \"B). The summer was so exciting it seems like I lived a lot during the summer months.\\n\\nThe reason it was unclear about there being so many memories in a summer is that the author found the summer to be so exciting, full of experiences, and eventful that it felt like a lot happened in a short amount of time.\\n\\nA). None of the above choices. - This is incorrect because option B is the appropriate choice and provides the reason.\\n\\nC). I was so tightly packed that I forgot. - This is incorrect because it doesn't address the question of how there were so many memories in a single summer, instead focusing on the author forgetting.\\n\\nD). My clothes were packed in tight cases and it was disconcerting. - This option is irrelevant and doesn't answer why there were many memories in a single summer.\",\n",
       " 'weight': 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['conversations'][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1709409094699,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "5zzyLG-Hs9Kj"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6J838o9AuIJV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huciEIlrvIsA"
   },
   "source": [
    "Generate Responses using our SFT (original) & DPO (trained) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1709409355881,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "xiJ91vdnvK2S",
    "outputId": "d74b5eff-2ab2-484f-c1b1-4c2420bef34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FastChat/fastchat/llm_judge\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%cd {os.getcwd()}/FastChat/fastchat/llm_judge\n",
    "# should be in FastChat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7156886,
     "status": "ok",
     "timestamp": 1709416576334,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "lOwro8HWvMEy",
    "outputId": "70814664-a14e-46d1-9f7e-37a36278035a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/mt_bench/model_answer/gemma-7b-Dolly15k-chatml.jsonl\n",
      "Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.19it/s]\n",
      "  0% 0/80 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100% 80/80 [1:58:58<00:00, 89.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# make sure that is the correct path\n",
    "model_path=\"jinhybr/gemma-7b-Dolly15k-chatml\"\n",
    "\n",
    "model_id=\"gemma-7b-Dolly15k-chatml\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1919618,
     "status": "ok",
     "timestamp": 1709419131637,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "0MZqXemKvb8M",
    "outputId": "949e32b4-c048-4bc0-89cd-eb1e224ff1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to data/mt_bench/model_answer/Mistral-7B-Instruct-v0.2.jsonl\n",
      "tokenizer_config.json: 100% 1.46k/1.46k [00:00<00:00, 6.36MB/s]\n",
      "tokenizer.model: 100% 493k/493k [00:00<00:00, 77.7MB/s]\n",
      "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 3.58MB/s]\n",
      "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 438kB/s]\n",
      "config.json: 100% 596/596 [00:00<00:00, 3.32MB/s]\n",
      "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 80.7MB/s]\n",
      "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
      "model-00001-of-00003.safetensors:   0% 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   1% 52.4M/4.94G [00:00<00:11, 440MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   2% 115M/4.94G [00:00<00:09, 494MB/s] \u001b[A\n",
      "model-00001-of-00003.safetensors:   4% 178M/4.94G [00:00<00:09, 511MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   5% 241M/4.94G [00:00<00:09, 501MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   6% 294M/4.94G [00:00<00:10, 452MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   7% 346M/4.94G [00:00<00:09, 471MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   8% 409M/4.94G [00:00<00:09, 490MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  10% 472M/4.94G [00:00<00:08, 505MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  11% 535M/4.94G [00:01<00:08, 515MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  12% 598M/4.94G [00:01<00:08, 521MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  13% 650M/4.94G [00:01<00:08, 521MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  14% 703M/4.94G [00:01<00:08, 521MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  15% 765M/4.94G [00:01<00:07, 523MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  17% 818M/4.94G [00:01<00:07, 523MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  18% 870M/4.94G [00:01<00:07, 513MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  19% 923M/4.94G [00:01<00:08, 490MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  20% 975M/4.94G [00:01<00:08, 481MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  21% 1.03G/4.94G [00:02<00:08, 475MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  22% 1.08G/4.94G [00:02<00:08, 478MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  23% 1.13G/4.94G [00:02<00:07, 483MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  24% 1.18G/4.94G [00:02<00:07, 489MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  25% 1.24G/4.94G [00:02<00:07, 485MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  26% 1.29G/4.94G [00:02<00:07, 496MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  27% 1.35G/4.94G [00:02<00:07, 510MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  29% 1.42G/4.94G [00:02<00:06, 514MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  30% 1.48G/4.94G [00:02<00:06, 516MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  31% 1.54G/4.94G [00:03<00:06, 522MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  32% 1.59G/4.94G [00:03<00:06, 520MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  33% 1.65G/4.94G [00:03<00:06, 520MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  34% 1.70G/4.94G [00:03<00:06, 520MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  35% 1.75G/4.94G [00:03<00:06, 519MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  37% 1.81G/4.94G [00:03<00:05, 522MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  38% 1.88G/4.94G [00:03<00:05, 528MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  39% 1.94G/4.94G [00:03<00:05, 528MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  41% 2.00G/4.94G [00:03<00:05, 531MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  42% 2.07G/4.94G [00:04<00:05, 533MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  43% 2.13G/4.94G [00:04<00:05, 533MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  44% 2.19G/4.94G [00:04<00:06, 450MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  45% 2.24G/4.94G [00:04<00:11, 234MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  46% 2.29G/4.94G [00:05<00:13, 191MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  47% 2.32G/4.94G [00:05<00:13, 190MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48% 2.35G/4.94G [00:05<00:12, 207MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  48% 2.39G/4.94G [00:05<00:10, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  49% 2.43G/4.94G [00:05<00:09, 267MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  50% 2.47G/4.94G [00:05<00:08, 294MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  51% 2.53G/4.94G [00:05<00:07, 333MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  52% 2.58G/4.94G [00:06<00:06, 377MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  53% 2.63G/4.94G [00:06<00:05, 413MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  54% 2.68G/4.94G [00:06<00:05, 442MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  55% 2.74G/4.94G [00:06<00:10, 218MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  56% 2.78G/4.94G [00:07<00:17, 125MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  57% 2.82G/4.94G [00:07<00:13, 152MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  58% 2.86G/4.94G [00:07<00:11, 179MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  59% 2.90G/4.94G [00:07<00:09, 209MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  60% 2.95G/4.94G [00:08<00:08, 230MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  60% 2.99G/4.94G [00:08<00:07, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  61% 3.03G/4.94G [00:08<00:07, 267MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  62% 3.07G/4.94G [00:08<00:06, 274MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  63% 3.10G/4.94G [00:08<00:06, 282MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  63% 3.14G/4.94G [00:08<00:06, 281MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  64% 3.17G/4.94G [00:08<00:06, 281MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  65% 3.20G/4.94G [00:08<00:06, 280MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  65% 3.23G/4.94G [00:08<00:06, 279MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  66% 3.26G/4.94G [00:09<00:05, 285MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  67% 3.29G/4.94G [00:09<00:05, 288MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  67% 3.32G/4.94G [00:09<00:05, 288MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  68% 3.36G/4.94G [00:09<00:05, 295MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  69% 3.39G/4.94G [00:09<00:05, 292MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  69% 3.42G/4.94G [00:09<00:05, 286MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  70% 3.45G/4.94G [00:09<00:05, 284MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  70% 3.48G/4.94G [00:09<00:05, 277MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  71% 3.51G/4.94G [00:09<00:05, 271MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  72% 3.54G/4.94G [00:10<00:05, 266MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  72% 3.58G/4.94G [00:10<00:05, 253MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  73% 3.61G/4.94G [00:10<00:05, 238MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  74% 3.64G/4.94G [00:10<00:05, 227MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  74% 3.67G/4.94G [00:10<00:06, 208MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  75% 3.70G/4.94G [00:10<00:06, 193MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  75% 3.72G/4.94G [00:11<00:06, 186MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  76% 3.74G/4.94G [00:11<00:06, 180MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  76% 3.76G/4.94G [00:11<00:06, 174MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  77% 3.80G/4.94G [00:11<00:05, 205MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  78% 3.84G/4.94G [00:11<00:04, 242MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  78% 3.88G/4.94G [00:11<00:03, 285MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  80% 3.93G/4.94G [00:11<00:03, 319MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  80% 3.97G/4.94G [00:11<00:03, 280MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  81% 4.01G/4.94G [00:12<00:03, 269MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  82% 4.04G/4.94G [00:12<00:03, 271MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  82% 4.07G/4.94G [00:12<00:03, 281MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  83% 4.10G/4.94G [00:12<00:02, 285MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  84% 4.14G/4.94G [00:12<00:02, 304MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85% 4.18G/4.94G [00:12<00:02, 318MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85% 4.23G/4.94G [00:12<00:02, 318MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  86% 4.27G/4.94G [00:13<00:03, 224MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  87% 4.32G/4.94G [00:13<00:02, 275MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  88% 4.37G/4.94G [00:13<00:01, 318MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  89% 4.41G/4.94G [00:13<00:01, 332MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  90% 4.46G/4.94G [00:13<00:01, 305MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  91% 4.51G/4.94G [00:13<00:01, 348MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  92% 4.56G/4.94G [00:13<00:01, 356MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  93% 4.60G/4.94G [00:14<00:01, 314MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  94% 4.65G/4.94G [00:14<00:01, 284MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  95% 4.68G/4.94G [00:14<00:00, 285MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  95% 4.72G/4.94G [00:14<00:00, 309MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  96% 4.76G/4.94G [00:14<00:00, 315MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  97% 4.81G/4.94G [00:14<00:00, 350MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  98% 4.85G/4.94G [00:14<00:00, 352MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors: 100% 4.94G/4.94G [00:15<00:00, 328MB/s]\n",
      "Downloading shards:  33% 1/3 [00:15<00:31, 15.60s/it]\n",
      "model-00002-of-00003.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   1% 41.9M/5.00G [00:00<00:11, 413MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   2% 105M/5.00G [00:00<00:10, 469MB/s] \u001b[A\n",
      "model-00002-of-00003.safetensors:   3% 157M/5.00G [00:00<00:11, 410MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   4% 199M/5.00G [00:00<00:12, 393MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   5% 252M/5.00G [00:00<00:11, 409MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   6% 304M/5.00G [00:00<00:10, 428MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   7% 357M/5.00G [00:00<00:10, 441MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   8% 409M/5.00G [00:00<00:10, 447MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   9% 461M/5.00G [00:02<00:59, 76.8MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  10% 503M/5.00G [00:04<01:30, 49.9MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  11% 545M/5.00G [00:04<01:08, 65.4MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  12% 577M/5.00G [00:04<00:55, 79.3MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  12% 619M/5.00G [00:04<00:42, 103MB/s] \u001b[A\n",
      "model-00002-of-00003.safetensors:  13% 661M/5.00G [00:04<00:33, 129MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  14% 692M/5.00G [00:05<00:28, 150MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  14% 724M/5.00G [00:05<00:25, 171MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  15% 755M/5.00G [00:05<00:22, 189MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  16% 786M/5.00G [00:05<00:20, 206MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  16% 818M/5.00G [00:05<00:19, 217MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  17% 849M/5.00G [00:05<00:18, 223MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  18% 881M/5.00G [00:05<00:17, 232MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  18% 912M/5.00G [00:05<00:17, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  19% 944M/5.00G [00:06<00:17, 233MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  20% 975M/5.00G [00:06<00:17, 233MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  20% 1.01G/5.00G [00:06<00:17, 233MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  21% 1.04G/5.00G [00:06<00:17, 221MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  21% 1.07G/5.00G [00:06<00:18, 216MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  22% 1.10G/5.00G [00:06<00:19, 203MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  22% 1.12G/5.00G [00:06<00:19, 200MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  23% 1.14G/5.00G [00:07<00:19, 198MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  23% 1.16G/5.00G [00:07<00:19, 196MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  24% 1.18G/5.00G [00:07<00:19, 196MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  24% 1.21G/5.00G [00:07<00:19, 192MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  25% 1.23G/5.00G [00:07<00:19, 196MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  25% 1.26G/5.00G [00:07<00:16, 225MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  26% 1.30G/5.00G [00:07<00:21, 172MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  27% 1.36G/5.00G [00:08<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  29% 1.43G/5.00G [00:08<00:10, 325MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  30% 1.49G/5.00G [00:08<00:09, 382MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  31% 1.55G/5.00G [00:08<00:08, 425MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  32% 1.60G/5.00G [00:08<00:08, 421MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  33% 1.66G/5.00G [00:08<00:08, 398MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  34% 1.70G/5.00G [00:08<00:09, 342MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  35% 1.74G/5.00G [00:08<00:09, 350MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  36% 1.78G/5.00G [00:09<00:08, 366MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  37% 1.84G/5.00G [00:09<00:08, 388MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  38% 1.88G/5.00G [00:09<00:08, 388MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  38% 1.92G/5.00G [00:09<00:07, 391MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  39% 1.96G/5.00G [00:09<00:07, 396MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  40% 2.00G/5.00G [00:09<00:07, 400MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  41% 2.04G/5.00G [00:09<00:07, 400MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  42% 2.10G/5.00G [00:09<00:06, 416MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  43% 2.14G/5.00G [00:09<00:06, 410MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  44% 2.18G/5.00G [00:10<00:07, 386MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  44% 2.22G/5.00G [00:10<00:07, 372MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  45% 2.26G/5.00G [00:10<00:07, 359MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  46% 2.31G/5.00G [00:10<00:07, 358MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  47% 2.35G/5.00G [00:10<00:07, 359MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  48% 2.39G/5.00G [00:10<00:07, 352MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  49% 2.44G/5.00G [00:10<00:06, 376MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  50% 2.49G/5.00G [00:10<00:06, 362MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  51% 2.53G/5.00G [00:11<00:06, 371MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  52% 2.58G/5.00G [00:11<00:06, 360MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  53% 2.63G/5.00G [00:11<00:06, 374MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  53% 2.67G/5.00G [00:11<00:06, 378MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  54% 2.72G/5.00G [00:11<00:07, 288MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  55% 2.76G/5.00G [00:11<00:08, 269MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  56% 2.79G/5.00G [00:11<00:08, 268MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  56% 2.82G/5.00G [00:12<00:08, 264MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  57% 2.85G/5.00G [00:12<00:07, 270MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  58% 2.89G/5.00G [00:12<00:07, 294MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  59% 2.95G/5.00G [00:12<00:06, 339MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  60% 3.00G/5.00G [00:12<00:05, 370MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  61% 3.05G/5.00G [00:12<00:04, 393MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  62% 3.09G/5.00G [00:12<00:04, 397MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  63% 3.14G/5.00G [00:14<00:22, 84.6MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  64% 3.18G/5.00G [00:14<00:16, 109MB/s] \u001b[A\n",
      "model-00002-of-00003.safetensors:  65% 3.23G/5.00G [00:14<00:12, 147MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  65% 3.27G/5.00G [00:14<00:09, 173MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  66% 3.31G/5.00G [00:14<00:08, 198MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  67% 3.36G/5.00G [00:14<00:07, 217MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  68% 3.40G/5.00G [00:14<00:06, 241MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  69% 3.44G/5.00G [00:15<00:06, 249MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  69% 3.47G/5.00G [00:15<00:05, 260MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  70% 3.50G/5.00G [00:15<00:05, 263MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  71% 3.53G/5.00G [00:15<00:05, 256MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  71% 3.57G/5.00G [00:15<00:05, 251MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  72% 3.60G/5.00G [00:15<00:05, 242MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  73% 3.63G/5.00G [00:15<00:05, 247MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  73% 3.66G/5.00G [00:16<00:05, 245MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  74% 3.69G/5.00G [00:16<00:05, 241MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  74% 3.72G/5.00G [00:16<00:05, 239MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  75% 3.75G/5.00G [00:16<00:05, 231MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  76% 3.79G/5.00G [00:16<00:05, 224MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  76% 3.82G/5.00G [00:16<00:05, 219MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  77% 3.85G/5.00G [00:16<00:05, 212MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  78% 3.88G/5.00G [00:17<00:05, 211MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  78% 3.91G/5.00G [00:17<00:05, 211MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  79% 3.94G/5.00G [00:17<00:04, 217MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  79% 3.97G/5.00G [00:17<00:04, 223MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  80% 4.01G/5.00G [00:17<00:04, 236MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  81% 4.05G/5.00G [00:17<00:03, 263MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  82% 4.09G/5.00G [00:17<00:03, 298MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  83% 4.13G/5.00G [00:17<00:02, 318MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  83% 4.17G/5.00G [00:18<00:02, 327MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  84% 4.22G/5.00G [00:18<00:02, 337MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  85% 4.27G/5.00G [00:18<00:01, 369MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  86% 4.31G/5.00G [00:18<00:01, 358MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  87% 4.36G/5.00G [00:18<00:01, 358MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  88% 4.40G/5.00G [00:18<00:01, 367MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  89% 4.45G/5.00G [00:18<00:01, 368MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  90% 4.49G/5.00G [00:18<00:01, 360MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  91% 4.53G/5.00G [00:19<00:01, 270MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  91% 4.56G/5.00G [00:19<00:01, 265MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  92% 4.59G/5.00G [00:19<00:01, 266MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  92% 4.62G/5.00G [00:19<00:01, 272MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  93% 4.67G/5.00G [00:19<00:01, 282MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  94% 4.71G/5.00G [00:19<00:00, 297MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  95% 4.74G/5.00G [00:19<00:00, 296MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  96% 4.78G/5.00G [00:19<00:00, 310MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  96% 4.81G/5.00G [00:20<00:00, 306MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  97% 4.84G/5.00G [00:20<00:00, 307MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  98% 4.89G/5.00G [00:20<00:00, 311MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  98% 4.92G/5.00G [00:20<00:00, 305MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:20<00:00, 240MB/s]\n",
      "Downloading shards:  67% 2/3 [00:36<00:18, 18.84s/it]\n",
      "model-00003-of-00003.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   1% 52.4M/4.54G [00:00<00:10, 446MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   3% 115M/4.54G [00:00<00:08, 494MB/s] \u001b[A\n",
      "model-00003-of-00003.safetensors:   4% 178M/4.54G [00:00<00:08, 515MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   5% 241M/4.54G [00:00<00:08, 517MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   6% 294M/4.54G [00:00<00:08, 503MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   8% 346M/4.54G [00:00<00:08, 482MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   9% 398M/4.54G [00:00<00:08, 475MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  10% 451M/4.54G [00:00<00:08, 474MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  11% 503M/4.54G [00:01<00:08, 469MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  12% 556M/4.54G [00:01<00:08, 469MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  13% 608M/4.54G [00:01<00:08, 467MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  15% 661M/4.54G [00:01<00:08, 462MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  16% 713M/4.54G [00:01<00:08, 465MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  17% 765M/4.54G [00:01<00:08, 462MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  18% 818M/4.54G [00:01<00:08, 465MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  19% 870M/4.54G [00:01<00:07, 461MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  20% 923M/4.54G [00:01<00:07, 463MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  21% 975M/4.54G [00:02<00:07, 466MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  23% 1.03G/4.54G [00:02<00:07, 468MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  24% 1.08G/4.54G [00:02<00:07, 480MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  25% 1.13G/4.54G [00:02<00:07, 483MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  26% 1.18G/4.54G [00:02<00:06, 485MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  27% 1.24G/4.54G [00:02<00:06, 491MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  28% 1.29G/4.54G [00:02<00:06, 491MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  30% 1.34G/4.54G [00:02<00:06, 491MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  31% 1.39G/4.54G [00:02<00:06, 490MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  32% 1.45G/4.54G [00:03<00:06, 490MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  33% 1.50G/4.54G [00:03<00:06, 487MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  34% 1.55G/4.54G [00:03<00:06, 484MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  35% 1.60G/4.54G [00:03<00:05, 491MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  37% 1.67G/4.54G [00:03<00:05, 509MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  38% 1.73G/4.54G [00:03<00:05, 518MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  39% 1.79G/4.54G [00:03<00:05, 526MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41% 1.86G/4.54G [00:03<00:05, 530MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42% 1.92G/4.54G [00:03<00:04, 530MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  44% 1.98G/4.54G [00:04<00:04, 532MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  45% 2.04G/4.54G [00:04<00:04, 534MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  46% 2.11G/4.54G [00:04<00:07, 339MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  48% 2.16G/4.54G [00:04<00:08, 294MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  48% 2.20G/4.54G [00:04<00:08, 276MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  49% 2.24G/4.54G [00:05<00:08, 258MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  50% 2.28G/4.54G [00:05<00:08, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  51% 2.31G/4.54G [00:05<00:09, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  51% 2.34G/4.54G [00:05<00:09, 232MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52% 2.37G/4.54G [00:05<00:09, 225MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  53% 2.40G/4.54G [00:05<00:09, 221MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  54% 2.43G/4.54G [00:05<00:09, 226MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  54% 2.46G/4.54G [00:06<00:09, 228MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55% 2.50G/4.54G [00:06<00:08, 234MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56% 2.53G/4.54G [00:06<00:08, 240MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56% 2.56G/4.54G [00:06<00:07, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  57% 2.59G/4.54G [00:06<00:07, 264MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  58% 2.63G/4.54G [00:06<00:06, 280MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59% 2.67G/4.54G [00:06<00:06, 305MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  60% 2.72G/4.54G [00:06<00:05, 315MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  61% 2.76G/4.54G [00:07<00:05, 333MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  62% 2.80G/4.54G [00:07<00:05, 324MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  63% 2.84G/4.54G [00:07<00:05, 308MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64% 2.88G/4.54G [00:07<00:05, 317MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64% 2.93G/4.54G [00:07<00:04, 333MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  65% 2.97G/4.54G [00:07<00:04, 348MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  67% 3.02G/4.54G [00:07<00:04, 372MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  68% 3.07G/4.54G [00:07<00:03, 398MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  69% 3.11G/4.54G [00:08<00:03, 398MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  70% 3.17G/4.54G [00:08<00:03, 431MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  71% 3.22G/4.54G [00:08<00:02, 451MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  72% 3.27G/4.54G [00:08<00:02, 426MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73% 3.32G/4.54G [00:08<00:02, 413MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  74% 3.37G/4.54G [00:08<00:02, 393MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  75% 3.41G/4.54G [00:08<00:02, 387MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  76% 3.45G/4.54G [00:08<00:02, 367MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  77% 3.49G/4.54G [00:08<00:02, 372MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  78% 3.53G/4.54G [00:09<00:02, 364MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  79% 3.58G/4.54G [00:09<00:02, 354MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  80% 3.62G/4.54G [00:09<00:02, 360MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  81% 3.66G/4.54G [00:09<00:02, 351MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82% 3.70G/4.54G [00:09<00:02, 347MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82% 3.74G/4.54G [00:09<00:02, 344MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  83% 3.79G/4.54G [00:09<00:02, 334MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  84% 3.83G/4.54G [00:09<00:02, 341MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  85% 3.87G/4.54G [00:10<00:02, 309MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  86% 3.91G/4.54G [00:10<00:02, 301MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  87% 3.96G/4.54G [00:10<00:01, 344MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88% 4.01G/4.54G [00:10<00:01, 361MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  89% 4.05G/4.54G [00:10<00:01, 347MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  90% 4.10G/4.54G [00:10<00:01, 382MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91% 4.14G/4.54G [00:10<00:01, 371MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  92% 4.19G/4.54G [00:10<00:00, 400MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  93% 4.24G/4.54G [00:11<00:00, 398MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94% 4.29G/4.54G [00:11<00:00, 403MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96% 4.34G/4.54G [00:11<00:00, 421MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  97% 4.39G/4.54G [00:11<00:00, 426MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98% 4.45G/4.54G [00:11<00:00, 425MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100% 4.54G/4.54G [00:11<00:00, 385MB/s]\n",
      "Downloading shards: 100% 3/3 [00:48<00:00, 16.26s/it]\n",
      "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.15s/it]\n",
      "generation_config.json: 100% 111/111 [00:00<00:00, 581kB/s]\n",
      "  0% 0/80 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1% 1/80 [00:29<38:17, 29.08s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2% 2/80 [00:45<27:57, 21.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4% 3/80 [00:57<22:05, 17.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5% 4/80 [01:02<15:33, 12.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6% 5/80 [01:33<23:46, 19.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8% 6/80 [01:41<19:01, 15.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9% 7/80 [02:10<24:00, 19.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10% 8/80 [03:06<37:47, 31.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11% 9/80 [03:13<28:04, 23.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12% 10/80 [03:26<23:58, 20.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14% 11/80 [03:31<17:53, 15.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15% 12/80 [04:00<22:30, 19.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16% 13/80 [04:47<31:20, 28.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18% 14/80 [05:08<28:19, 25.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19% 15/80 [05:45<31:35, 29.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20% 16/80 [05:52<24:04, 22.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21% 17/80 [06:23<26:26, 25.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22% 18/80 [06:55<28:07, 27.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24% 19/80 [07:21<27:22, 26.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25% 20/80 [07:31<21:44, 21.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26% 21/80 [07:59<23:17, 23.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28% 22/80 [08:28<24:11, 25.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29% 23/80 [08:48<22:27, 23.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30% 24/80 [09:11<21:51, 23.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31% 25/80 [09:53<26:32, 28.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32% 26/80 [10:10<22:47, 25.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34% 27/80 [10:36<22:47, 25.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35% 28/80 [10:59<21:24, 24.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36% 29/80 [11:22<20:35, 24.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38% 30/80 [11:52<21:36, 25.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39% 31/80 [12:23<22:29, 27.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40% 32/80 [13:02<24:50, 31.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41% 33/80 [13:05<17:37, 22.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42% 34/80 [13:40<20:12, 26.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44% 35/80 [14:01<18:32, 24.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45% 36/80 [14:06<13:49, 18.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46% 37/80 [14:17<11:51, 16.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48% 38/80 [14:40<12:49, 18.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49% 39/80 [15:20<17:03, 24.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50% 40/80 [15:33<14:08, 21.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51% 41/80 [15:39<10:54, 16.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52% 42/80 [16:00<11:24, 18.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54% 43/80 [16:11<09:49, 15.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55% 44/80 [16:45<12:47, 21.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56% 45/80 [17:08<12:47, 21.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57% 46/80 [17:18<10:18, 18.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59% 47/80 [17:27<08:27, 15.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60% 48/80 [18:14<13:15, 24.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61% 49/80 [18:17<09:32, 18.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62% 50/80 [18:49<11:12, 22.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64% 51/80 [19:15<11:24, 23.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65% 52/80 [19:28<09:34, 20.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66% 53/80 [19:55<10:00, 22.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68% 54/80 [20:28<11:01, 25.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69% 55/80 [20:59<11:18, 27.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70% 56/80 [21:09<08:51, 22.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71% 57/80 [21:14<06:27, 16.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72% 58/80 [21:29<05:58, 16.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74% 59/80 [21:46<05:47, 16.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75% 60/80 [22:41<09:22, 28.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76% 61/80 [22:48<06:54, 21.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78% 62/80 [23:15<07:02, 23.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79% 63/80 [23:42<06:57, 24.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80% 64/80 [24:13<07:00, 26.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81% 65/80 [24:49<07:19, 29.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82% 66/80 [25:29<07:34, 32.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84% 67/80 [25:40<05:37, 25.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85% 68/80 [26:14<05:42, 28.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86% 69/80 [26:27<04:23, 23.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88% 70/80 [26:57<04:15, 25.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89% 71/80 [27:37<04:28, 29.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90% 72/80 [27:53<03:26, 25.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91% 73/80 [28:16<02:53, 24.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92% 74/80 [28:36<02:20, 23.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94% 75/80 [29:11<02:15, 27.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95% 76/80 [29:20<01:25, 21.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96% 77/80 [29:27<00:51, 17.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98% 78/80 [29:56<00:41, 20.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99% 79/80 [30:12<00:19, 19.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100% 80/80 [30:52<00:00, 23.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Change this to where you saved the model during training, remember our current directory is FastChat/\n",
    "model_path=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model_id=\"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# generate model answer\n",
    "!python gen_model_answer.py --model-id {model_id} --model-path {model_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3205925,
     "status": "ok",
     "timestamp": 1709423708024,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "meapOAkTL4Ia",
    "outputId": "ba9540f1-144f-4110-b858-c6fdab3723be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "{\n",
      "    \"bench_name\": \"mt_bench\",\n",
      "    \"mode\": \"pairwise-all\",\n",
      "    \"judge\": \"gpt-4-1106-preview\",\n",
      "    \"baseline\": null,\n",
      "    \"model_list\": [\n",
      "        \"gemma-7b-Dolly15k-chatml\",\n",
      "        \"Mistral-7B-Instruct-v0.2\"\n",
      "    ],\n",
      "    \"total_num_questions\": 80,\n",
      "    \"total_num_matches\": 160,\n",
      "    \"output_path\": \"data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\"\n",
      "}\n",
      "Evaluating the following models.\n",
      "  0% 0/160 [00:00<?, ?it/s]question: 81, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  1% 1/160 [00:13<35:26, 13.37s/it]question: 82, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  1% 2/160 [00:27<35:53, 13.63s/it]question: 83, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  2% 3/160 [00:29<21:51,  8.35s/it]question: 84, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  2% 4/160 [00:31<15:09,  5.83s/it]question: 85, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  3% 5/160 [00:34<12:13,  4.73s/it]question: 86, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  4% 6/160 [00:46<18:56,  7.38s/it]question: 87, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  4% 7/160 [01:07<30:09, 11.83s/it]question: 88, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  5% 8/160 [01:24<34:12, 13.50s/it]question: 89, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  6% 9/160 [01:26<24:43,  9.82s/it]question: 90, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  6% 10/160 [01:51<36:06, 14.44s/it]question: 91, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  7% 11/160 [02:14<42:54, 17.28s/it]question: 92, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  8% 12/160 [02:16<31:14, 12.67s/it]question: 93, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  8% 13/160 [02:31<32:04, 13.09s/it]question: 94, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  9% 14/160 [02:43<31:40, 13.02s/it]question: 95, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      "  9% 15/160 [02:45<23:05,  9.55s/it]question: 96, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 10% 16/160 [02:59<26:32, 11.06s/it]question: 97, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 11% 17/160 [03:02<20:09,  8.46s/it]question: 98, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 11% 18/160 [03:19<26:25, 11.16s/it]question: 99, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: error, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 12% 19/160 [04:09<53:09, 22.62s/it]question: 100, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 12% 20/160 [04:10<38:02, 16.30s/it]question: 131, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 13% 21/160 [04:25<37:03, 15.99s/it]question: 132, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 14% 22/160 [04:27<27:05, 11.78s/it]question: 133, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 14% 23/160 [04:29<20:01,  8.77s/it]question: 134, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 15% 24/160 [04:31<15:05,  6.66s/it]question: 135, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 16% 25/160 [04:47<21:02,  9.35s/it]question: 136, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 16% 26/160 [05:04<26:02, 11.66s/it]question: 137, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 17% 27/160 [05:15<25:31, 11.51s/it]question: 138, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 18% 28/160 [05:28<26:35, 12.09s/it]question: 139, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 18% 29/160 [05:42<27:31, 12.61s/it]question: 140, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 19% 30/160 [06:02<32:00, 14.77s/it]question: 141, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 19% 31/160 [06:04<23:38, 11.00s/it]question: 142, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: error, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 20% 32/160 [06:30<32:51, 15.40s/it]question: 143, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 21% 33/160 [06:43<31:22, 14.82s/it]question: 144, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 21% 34/160 [06:46<23:18, 11.10s/it]question: 145, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 22% 35/160 [07:03<27:20, 13.13s/it]question: 146, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 22% 36/160 [07:31<35:50, 17.34s/it]question: 147, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 23% 37/160 [07:33<26:21, 12.86s/it]question: 148, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 24% 38/160 [07:49<28:04, 13.81s/it]question: 149, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 24% 39/160 [07:56<23:37, 11.71s/it]question: 150, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 25% 40/160 [07:58<17:28,  8.74s/it]question: 151, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 26% 41/160 [08:04<16:03,  8.10s/it]question: 152, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 26% 42/160 [08:06<12:07,  6.17s/it]question: 153, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 27% 43/160 [08:14<13:19,  6.83s/it]question: 154, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 28% 44/160 [08:17<10:37,  5.49s/it]question: 155, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 28% 45/160 [08:31<15:22,  8.02s/it]question: 156, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 29% 46/160 [08:33<11:56,  6.28s/it]question: 157, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 29% 47/160 [08:43<13:57,  7.42s/it]question: 158, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 30% 48/160 [08:45<10:57,  5.87s/it]question: 159, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 31% 49/160 [09:04<17:58,  9.72s/it]question: 160, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2')\n",
      " 31% 50/160 [09:06<13:50,  7.55s/it]question: 101, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 32% 51/160 [09:20<16:47,  9.24s/it]question: 102, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 32% 52/160 [09:50<28:14, 15.69s/it]question: 103, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 33% 53/160 [10:06<28:08, 15.78s/it]question: 104, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 34% 54/160 [10:17<25:21, 14.35s/it]question: 105, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 34% 55/160 [10:50<34:49, 19.90s/it]question: 106, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 35% 56/160 [11:12<35:39, 20.57s/it]question: 107, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 36% 57/160 [11:25<31:22, 18.28s/it]question: 108, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 36% 58/160 [11:41<29:53, 17.59s/it]question: 109, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 37% 59/160 [11:57<28:37, 17.01s/it]question: 110, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 38% 60/160 [12:21<31:45, 19.05s/it]question: 111, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 38% 61/160 [12:57<39:52, 24.17s/it]question: 112, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 39% 62/160 [13:08<33:11, 20.32s/it]question: 113, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 39% 63/160 [13:50<43:09, 26.70s/it]question: 114, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 40% 64/160 [14:10<39:27, 24.66s/it]question: 115, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 41% 65/160 [14:34<38:53, 24.56s/it]question: 116, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 41% 66/160 [15:15<46:28, 29.67s/it]question: 117, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 42% 67/160 [15:34<40:44, 26.28s/it]question: 118, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 42% 68/160 [16:23<50:50, 33.16s/it]question: 119, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 43% 69/160 [16:35<40:42, 26.84s/it]question: 120, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 44% 70/160 [16:54<36:51, 24.57s/it]question: 121, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 44% 71/160 [17:31<41:34, 28.03s/it]question: 122, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 45% 72/160 [18:22<51:16, 34.96s/it]question: 123, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 46% 73/160 [18:45<45:26, 31.33s/it]question: 124, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 46% 74/160 [19:01<38:35, 26.93s/it]question: 125, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 47% 75/160 [19:28<38:14, 27.00s/it]question: 126, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 48% 76/160 [19:55<37:44, 26.96s/it]question: 127, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 48% 77/160 [20:14<34:01, 24.60s/it]question: 128, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 49% 78/160 [20:29<29:42, 21.74s/it]question: 129, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 49% 79/160 [20:59<32:33, 24.11s/it]question: 130, turn: 1, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1')\n",
      " 50% 80/160 [21:26<33:18, 24.98s/it]question: 81, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 51% 81/160 [21:57<35:04, 26.64s/it]question: 82, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 51% 82/160 [22:07<28:23, 21.84s/it]question: 83, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 52% 83/160 [22:27<27:23, 21.35s/it]question: 84, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 52% 84/160 [22:46<25:56, 20.49s/it]question: 85, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 53% 85/160 [23:39<38:00, 30.41s/it]question: 86, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 54% 86/160 [24:05<35:45, 28.99s/it]question: 87, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 54% 87/160 [24:28<33:03, 27.18s/it]question: 88, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 55% 88/160 [24:55<32:26, 27.03s/it]question: 89, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 56% 89/160 [25:16<29:53, 25.27s/it]question: 90, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 56% 90/160 [25:32<26:17, 22.54s/it]question: 91, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 57% 91/160 [25:55<25:53, 22.52s/it]question: 92, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 57% 92/160 [26:20<26:32, 23.42s/it]question: 93, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 58% 93/160 [26:38<24:21, 21.81s/it]question: 94, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 59% 94/160 [27:15<28:53, 26.27s/it]question: 95, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 59% 95/160 [28:02<35:08, 32.43s/it]question: 96, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 60% 96/160 [28:22<30:44, 28.82s/it]question: 97, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 61% 97/160 [28:46<28:38, 27.28s/it]question: 98, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 61% 98/160 [29:17<29:26, 28.49s/it]question: 99, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 62% 99/160 [29:49<30:02, 29.55s/it]question: 100, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 62% 100/160 [30:05<25:23, 25.40s/it]question: 131, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 63% 101/160 [30:22<22:36, 22.99s/it]question: 132, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 64% 102/160 [30:35<19:24, 20.08s/it]question: 133, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 64% 103/160 [30:48<16:53, 17.78s/it]question: 134, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 65% 104/160 [31:00<15:00, 16.07s/it]question: 135, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 66% 105/160 [31:17<15:03, 16.42s/it]question: 136, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 66% 106/160 [31:35<15:05, 16.76s/it]question: 137, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 67% 107/160 [31:48<13:50, 15.66s/it]question: 138, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 68% 108/160 [32:03<13:34, 15.67s/it]question: 139, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 68% 109/160 [32:34<17:09, 20.19s/it]question: 140, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 69% 110/160 [33:04<19:11, 23.03s/it]question: 141, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 69% 111/160 [33:29<19:22, 23.72s/it]question: 142, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 70% 112/160 [34:01<20:51, 26.07s/it]question: 143, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 71% 113/160 [34:28<20:42, 26.43s/it]question: 144, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 71% 114/160 [34:56<20:37, 26.91s/it]question: 145, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 72% 115/160 [35:49<26:03, 34.74s/it]question: 146, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 72% 116/160 [36:08<21:58, 29.96s/it]question: 147, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 73% 117/160 [36:34<20:36, 28.76s/it]question: 148, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 74% 118/160 [36:50<17:28, 24.97s/it]question: 149, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 74% 119/160 [37:11<16:18, 23.87s/it]question: 150, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 75% 120/160 [37:44<17:39, 26.49s/it]question: 151, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 76% 121/160 [38:10<17:05, 26.28s/it]question: 152, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 76% 122/160 [38:36<16:34, 26.17s/it]question: 153, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 77% 123/160 [38:58<15:24, 24.98s/it]question: 154, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 78% 124/160 [39:32<16:39, 27.75s/it]question: 155, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 78% 125/160 [40:05<17:10, 29.44s/it]question: 156, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 79% 126/160 [40:36<16:48, 29.66s/it]question: 157, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 79% 127/160 [40:56<14:48, 26.93s/it]question: 158, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 80% 128/160 [41:19<13:46, 25.82s/it]question: 159, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 81% 129/160 [41:49<13:59, 27.07s/it]question: 160, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-v2-multi-turn')\n",
      " 81% 130/160 [42:17<13:37, 27.25s/it]question: 101, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 82% 131/160 [42:35<11:48, 24.45s/it]question: 102, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 82% 132/160 [42:55<10:49, 23.20s/it]question: 103, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 83% 133/160 [43:27<11:39, 25.91s/it]question: 104, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 84% 134/160 [43:46<10:19, 23.82s/it]question: 105, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 84% 135/160 [44:08<09:38, 23.15s/it]question: 106, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 85% 136/160 [44:32<09:20, 23.33s/it]question: 107, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 86% 137/160 [44:48<08:06, 21.16s/it]question: 108, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 86% 138/160 [45:05<07:20, 20.01s/it]question: 109, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_1, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 87% 139/160 [45:20<06:26, 18.40s/it]question: 110, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 88% 140/160 [45:36<05:53, 17.67s/it]question: 111, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 88% 141/160 [45:54<05:41, 17.95s/it]question: 112, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_1, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 89% 142/160 [46:14<05:32, 18.50s/it]question: 113, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 89% 143/160 [46:27<04:48, 16.96s/it]question: 114, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 90% 144/160 [46:44<04:31, 16.98s/it]question: 115, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 91% 145/160 [47:03<04:22, 17.51s/it]question: 116, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 91% 146/160 [47:38<05:17, 22.66s/it]question: 117, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 92% 147/160 [47:58<04:43, 21.79s/it]question: 118, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 92% 148/160 [48:14<04:03, 20.25s/it]question: 119, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 93% 149/160 [48:37<03:49, 20.87s/it]question: 120, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 94% 150/160 [49:07<03:56, 23.63s/it]question: 121, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 94% 151/160 [49:32<03:38, 24.24s/it]question: 122, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 95% 152/160 [49:58<03:16, 24.56s/it]question: 123, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 96% 153/160 [50:21<02:50, 24.31s/it]question: 124, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 96% 154/160 [50:37<02:10, 21.76s/it]question: 125, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: model_2, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 97% 155/160 [51:11<02:06, 25.35s/it]question: 126, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 98% 156/160 [51:37<01:41, 25.48s/it]question: 127, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 98% 157/160 [52:08<01:21, 27.30s/it]question: 128, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 99% 158/160 [52:34<00:53, 26.79s/it]question: 129, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: tie, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      " 99% 159/160 [52:51<00:23, 23.83s/it]question: 130, turn: 2, model_1: gemma-7b-Dolly15k-chatml, model_2: Mistral-7B-Instruct-v0.2, g1_winner: tie, g2_winner: model_2, judge: ('gpt-4-1106-preview', 'pair-math-v1-multi-turn')\n",
      "100% 160/160 [53:21<00:00, 20.01s/it]\n"
     ]
    }
   ],
   "source": [
    "open_ai_key=\"\" # replace with your openai key\n",
    "\n",
    "# Pairwise comparison of the two models using OpenAI's GPT-4 Turbo\n",
    "!OPENAI_API_KEY={open_ai_key} python gen_judgment.py --model-list \"gemma-7b-Dolly15k-chatml\" \"Mistral-7B-Instruct-v0.2\" --judge-model \"gpt-4-1106-preview\" --mode \"pairwise-all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1709423780283,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "1HstJ2yVVxfo"
   },
   "outputs": [],
   "source": [
    "res = \"./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1536,
     "status": "ok",
     "timestamp": 1709423844672,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "d46G0cXxmRaR",
    "outputId": "07c387f9-0656-4f83-b709-f8b0a90523a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: pairwise-all\n",
      "Input file: ./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\n",
      "| model                    |   win |   loss |   tie |   win_rate |   loss_rate |   win_rate_adjusted |\n",
      "|:-------------------------|------:|-------:|------:|-----------:|------------:|--------------------:|\n",
      "| Mistral-7B-Instruct-v0.2 |   106 |      8 |    44 |  0.670886  |   0.0506329 |            0.810127 |\n",
      "| gemma-7b-Dolly15k-chatml |     8 |    106 |    44 |  0.0506329 |   0.670886  |            0.189873 |\n"
     ]
    }
   ],
   "source": [
    "!python show_result.py --input-file {res} --model-list \"gemma-7b-Dolly15k-chatml\" \"Mistral-7B-Instruct-v0.2\" --judge-model \"gpt-4-1106-preview\" --mode \"pairwise-all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1709423928354,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "q4Tuq0XTmiet",
    "outputId": "0eaf24cf-a233-4fd8-cb3d-468548613e5d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt: THE ABOVE CODE NOT SHOW IMAGE\n",
    "\n",
    "# Display the image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the image\n",
    "image = plt.imread(\"win_rate_gpt-4-1106-preview.png\")\n",
    "\n",
    "# Create a figure and display the image\n",
    "fig = plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34211,
     "status": "ok",
     "timestamp": 1709424107174,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "2VjtBQA8myCk",
    "outputId": "b710fc72-963f-4499-86ab-f0b6f2958808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# prompt: SAVE THIS FILE INTO GOOGLE DRIVE \" res = \"./data/mt_bench/model_judgment/gpt-4-1106-preview_pair.jsonl\"\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1709424125822,
     "user": {
      "displayName": "Tao Jin",
      "userId": "05811861299079065602"
     },
     "user_tz": 300
    },
    "id": "UZZawzfMnBp0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the file to Google Drive\n",
    "!cp {res} \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEJoDAymniEB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZaSyv/qOy9j+YAvQnwv99",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04ab59bc6a4c42fc9a00f982b86f4ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f0e2689687c42b5b6e6b2f912c791b0",
      "placeholder": "​",
      "style": "IPY_MODEL_e31d0391df1e46639a11a9ad2be849b2",
      "value": " 4/4 [00:09&lt;00:00,  2.23s/it]"
     }
    },
    "0c743f33a98e4df4b82443798c68e079": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10da2929d4854ef3bb2e472b264e6d43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22aed5cd75734b9ca913770a558967ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46481ee5621a42708d58eaab50795b5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f0e2689687c42b5b6e6b2f912c791b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f448c0b115047bb95823e2793e0cb80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf2bd3529398440a8ac4a10e44e3eb3d",
       "IPY_MODEL_6dc5db10fb494414b301cc848a55a7a1",
       "IPY_MODEL_04ab59bc6a4c42fc9a00f982b86f4ce5"
      ],
      "layout": "IPY_MODEL_46481ee5621a42708d58eaab50795b5b"
     }
    },
    "6dc5db10fb494414b301cc848a55a7a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c743f33a98e4df4b82443798c68e079",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22aed5cd75734b9ca913770a558967ef",
      "value": 4
     }
    },
    "bf2bd3529398440a8ac4a10e44e3eb3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10da2929d4854ef3bb2e472b264e6d43",
      "placeholder": "​",
      "style": "IPY_MODEL_cca4b067586e483593154a0e95d035ee",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "cca4b067586e483593154a0e95d035ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e31d0391df1e46639a11a9ad2be849b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
